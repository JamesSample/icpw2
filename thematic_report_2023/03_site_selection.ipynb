{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3602f8d-bbc2-4147-9d56-d353f88889aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import utils\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94d5bf-e946-4527-951f-da40b9a3e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff336e-3864-45c2-88dc-42de996c6fec",
   "metadata": {},
   "source": [
    "# ICPW Thematic report 2023\n",
    "\n",
    "## Part A: Site selection\n",
    "\n",
    "## 1. Get stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d05402e-c19a-46a9-a409-561479ff3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ICPW stations\n",
    "xl_path = r\"../../../all_icpw_sites_mar_2023.xlsx\"\n",
    "stn_df = pd.read_excel(xl_path, sheet_name=\"all_icpw_stns\")\n",
    "print(f\"There are {len(stn_df)} stations in the ICPW project.\")\n",
    "display(stn_df.head())\n",
    "nivapy.spatial.quickmap(stn_df, cluster=True, popup=\"station_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ac358-ab43-439e-a8ce-e985cb858964",
   "metadata": {},
   "source": [
    "## 2. Get chemistry\n",
    "\n",
    "### 2.1. Basic parameters\n",
    "\n",
    " * Assume LOD values are equal to the LOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92870b-47f4-449f-af9c-8367a335f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify time period of interest\n",
    "st_dt = \"1990-01-01\"\n",
    "end_dt = \"2020-12-31\"\n",
    "\n",
    "# Specify RESA2 parameters of interest\n",
    "params = [\"SO4\", \"NO3-N\", \"NH4-N\", \"Cl\", \"Ca\", \"Mg\", \"Na\", \"K\", \"pH\", \"TOC\"]\n",
    "\n",
    "# Get available parameters\n",
    "par_df = nivapy.da.select_resa_station_parameters(stn_df, st_dt, end_dt, eng)\n",
    "par_df = par_df.query(\"parameter_name in @params\")\n",
    "par_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e016bd-b573-4a80-a10d-43fc28e4ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "wc_df, dup_df = nivapy.da.select_resa_water_chemistry(\n",
    "    stn_df,\n",
    "    par_df,\n",
    "    st_dt,\n",
    "    end_dt,\n",
    "    eng,\n",
    "    lod_flags=False,\n",
    "    drop_dups=True,\n",
    ")\n",
    "\n",
    "# Only consider samples within 1 m of surface\n",
    "wc_df = wc_df.query(\"depth2 < 1\")\n",
    "del wc_df[\"depth1\"], wc_df[\"depth2\"]\n",
    "\n",
    "wc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a6de4-74b2-4a0b-b770-a4723ea126eb",
   "metadata": {},
   "source": [
    "### 2.2. Derived parameters\n",
    "\n",
    "Using the \"basic\" parameters above:\n",
    "\n",
    " * Convert to microequivalents per litre where relevant\n",
    " \n",
    " * Calculate (Ca + Mg) in ueq/l\n",
    " \n",
    " * Calculate [H+] from pH\n",
    " \n",
    " * Calculate SAA (sum of acid anions) as $(Cl + SO_4 + NO_3)$ (all expressed in ueq/l)\n",
    " \n",
    " * Calculate ANC as $(Ca + Mg + Na + K + NH_4) - (Cl + SO_4 + NO_3)$ (all expressed in ueq/l)\n",
    " \n",
    " * Calculate organic anions using the model of [Hruška et al. (2003)](https://doi.org/10.1021/es0201552)\n",
    " \n",
    " * Calculate bicarbonate from the ion balance as $ANC + H - OrgAnions$ (all expressed in ueq/l)\n",
    " \n",
    "See the issue [here](https://github.com/JamesSample/icpw2/issues/3) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89839453-cd2b-4e49-baa4-2fb16246c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ueq/l\n",
    "ueq_cols = [\n",
    "    \"SO4_mg/l\",\n",
    "    \"NO3-N_µg/l N\",\n",
    "    \"NH4-N_µg/l N\",\n",
    "    \"Cl_mg/l\",\n",
    "    \"Ca_mg/l\",\n",
    "    \"Mg_mg/l\",\n",
    "    \"Na_mg/l\",\n",
    "    \"K_mg/l\",\n",
    "]\n",
    "for col in ueq_cols:\n",
    "    wc_df = utils.convert_to_microequivalents(wc_df, col)\n",
    "\n",
    "# Ca + Mg\n",
    "wc_df[\"CaMg_µeq/l\"] = wc_df[\"Ca_µeq/l\"] + wc_df[\"Mg_µeq/l\"]\n",
    "\n",
    "# H+ from pH\n",
    "wc_df[\"H_µeq/l\"] = 1e6 * 10 ** -wc_df[\"pH_\"]\n",
    "\n",
    "# Calculate SAA\n",
    "wc_df[\"SAA_µeq/l\"] = wc_df[\"Cl_µeq/l\"] + wc_df[\"SO4_µeq/l\"] + wc_df[\"NO3-N_µeq/l\"]\n",
    "\n",
    "# Calculate ANC\n",
    "wc_df = utils.calculate_anc(wc_df, anc_oaa=False)\n",
    "\n",
    "# Organic anions. See\n",
    "# https://github.com/JamesSample/icpw2/issues/4#issuecomment-1528806056\n",
    "# for choice of 'site_density'\n",
    "wc_df = utils.calculate_organic_anions(wc_df, site_density=16.6)\n",
    "\n",
    "# HCO3\n",
    "wc_df = utils.calculate_bicarbonate(wc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584da65-36be-4fa1-a516-18c0ad61c3ec",
   "metadata": {},
   "source": [
    "## 3. Raw data distributions by country\n",
    "\n",
    "For each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694f272-05f6-4410-a190-f2d920b4efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join country attribute\n",
    "df = pd.merge(wc_df, stn_df[[\"station_id\", \"country\"]], how=\"left\", on=\"station_id\")\n",
    "\n",
    "# Pars to plot\n",
    "pars = [col for col in df.columns if \"eq/l\" in col] + [\"TOC_mg C/l\"]\n",
    "\n",
    "# Reshape\n",
    "df = df[[\"country\"] + pars]\n",
    "df = df.melt(id_vars=[\"country\"])\n",
    "\n",
    "# Plot\n",
    "g = sn.catplot(\n",
    "    data=df,\n",
    "    x=\"country\",\n",
    "    y=\"value\",\n",
    "    row=\"variable\",\n",
    "    kind=\"box\",\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    height=4,\n",
    "    aspect=3,\n",
    ")\n",
    "g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbd85c-1af8-4f7a-ba8e-3d4f3511d8d2",
   "metadata": {},
   "source": [
    "## 4. Data cleaning\n",
    "\n",
    "### 4.1. Filter stations and countries\n",
    "\n",
    "Based on e-mail discussion with Rolf and Heleen 26.04.2023:\n",
    "\n",
    " * Remove data from Moldova (`MD01` and `MD02`), as we suspect these sites are affected by mining\n",
    " \n",
    " * Remove `DE04`, `DE09`, `DE14`, `DE15`, `DE21`, `DE25`, `DE30` and `DE31`, as we suspect they are also affected by mining and/or liming (see also e-mail from Rolf received 29.04.2023)\n",
    " \n",
    " * Remove data from Latvia and Estonia, due to very different geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffa870-bab6-4fdf-a85e-ef4c4af674c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = len(wc_df)\n",
    "print(len1, \"samples in the raw dataset.\")\n",
    "\n",
    "# Remove stations in Germany and Moldova affected by mining\n",
    "wc_df = wc_df.query(\n",
    "    \"station_code not in ('MD01', 'MD02', 'DE04', 'DE09', 'DE14', 'DE15', 'DE21', 'DE25', 'DE30', 'DE31')\"\n",
    ")\n",
    "\n",
    "# Latvia and Estonia have unusual geology and should be removed from\n",
    "# this analysis. See e-mail from Heleen received 26.04.2023 at 10.26\n",
    "exclude_list = stn_df.query(\"country in ('Latvia', 'Estonia')\")[\"station_code\"].tolist()\n",
    "wc_df = wc_df.query(\"station_code not in @exclude_list\")\n",
    "\n",
    "len2 = len(wc_df)\n",
    "print(len1 - len2, \"samples excluded.\")\n",
    "print(len2, \"samples remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b8883-9a25-47bc-97e8-bd85ef99bf2c",
   "metadata": {},
   "source": [
    "### 4.2. Filter outliers\n",
    "\n",
    "The code below uses a simple approach to remove outliers based on the \"double MAD\" method (see e.g. [here](http://eurekastatistics.com/using-the-median-absolute-deviation-to-find-outliers/)). Note that this method does not account for autocorrelation in the series, as it just considers the values not the temporal order. It is also not good at detecting step-changes where there are plenty of values either side of the \"step\".\n",
    "\n",
    "In general, I prefer to use robust statistics to reduce the influence of outliers at the analysis stage, rather than filtering them out of the dataset *a priori*. I have therefore set a **conservative threshold** for removing outliers in the code below: a typical value would be `thresh=3.5`, whereas I have used `thresh=5`. My code will therefore only filter out the most extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86708a9c-3ac7-40ac-9a83-bcf03e4271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 'long'\n",
    "id_cols = [\"station_id\", \"station_code\", \"station_name\", \"sample_date\"]\n",
    "pars = [col for col in wc_df.columns if \"eq/l\" in col] + [\"TOC_mg C/l\"]\n",
    "df = wc_df[id_cols + pars].copy()\n",
    "df = df.melt(id_vars=id_cols, var_name=\"parameter\").dropna()\n",
    "\n",
    "# Test each series for outliers\n",
    "df_list = []\n",
    "stn_list = df[\"station_id\"].unique().tolist()\n",
    "\n",
    "for stn_id in stn_list:\n",
    "    for par in pars:\n",
    "        stn_par_df = df.query(\"(station_id == @stn_id) and (parameter == @par)\").copy()\n",
    "        if len(stn_par_df) > 0:\n",
    "            stn_par_df[\"outlier\"] = utils.double_mad_from_median(\n",
    "                stn_par_df[\"value\"], thresh=5\n",
    "            )\n",
    "            df_list.append(stn_par_df)\n",
    "df = pd.concat(df_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903af49-eab7-4e44-8732-f28a57cb09ed",
   "metadata": {},
   "source": [
    "As a sense check, the code below selects 9 series with outliers at random from the dataset and shows the time series, with outliers identified in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7f136-cfbd-48c5-9f7a-88817aff795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample series with outliers to check the filtering is reasonable\n",
    "rand_df = (\n",
    "    df.query(\"outlier == True\")[[\"station_id\", \"station_code\", \"parameter\"]]\n",
    "    .drop_duplicates()\n",
    "    .sample(n=9)\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for idx, row in rand_df.iterrows():\n",
    "    stn_id = row[\"station_id\"]\n",
    "    stn_code = row[\"station_code\"]\n",
    "    par = row[\"parameter\"]\n",
    "    data_df = df.query(\"(station_id == @stn_id) and (parameter == @par)\").copy()\n",
    "    data_df.sort_values(\"sample_date\", inplace=True)\n",
    "    data_df[\"outlier_col\"] = [\"r\" if i else \"k\" for i in data_df[\"outlier\"]]\n",
    "    axes[idx].plot(data_df[\"sample_date\"], data_df[\"value\"], \"k-\")\n",
    "    axes[idx].scatter(\n",
    "        data_df[\"sample_date\"], data_df[\"value\"], c=data_df[\"outlier_col\"].tolist()\n",
    "    )\n",
    "    axes[idx].set_title(f\"{par} at station {stn_code}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29188a72-6bfc-4e95-940b-e55a7c7ad767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers and reshape to 'wide'\n",
    "df = df.query(\"outlier == False\")\n",
    "del df[\"outlier\"]\n",
    "df = df.groupby(id_cols + [\"parameter\"]).mean().reset_index()\n",
    "wc_df = df.pivot(columns=\"parameter\", index=id_cols, values=\"value\").reset_index()\n",
    "wc_df.dropna(subset=pars, how=\"all\", inplace=True)\n",
    "wc_df.columns.name = \"\"\n",
    "\n",
    "print(len(wc_df), \"samples remain after filtering outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb449912-dc00-4a38-91c1-30d0eff2e975",
   "metadata": {},
   "source": [
    "### 4.3. Ion balance\n",
    "\n",
    "In a comment [here](https://github.com/JamesSample/icpw2/issues/4#issuecomment-1522126141), Rolf suggested calculating the ion balance and discarding samples where the difference is >20%. However, we have already calculated bicarbonate using the ion balance ([here](https://github.com/JamesSample/icpw2/issues/3#issuecomment-1521604595)), so the ion balance in this dataset is by definition zero, *except in cases where the estimate for HCO3 is initially negative* (in which case it is [set back to zero](https://github.com/JamesSample/icpw2/blob/e0192bc439618ed9d3c10c7adbc1d28836c6a456/thematic_report_2023/utils.py#L477)). See the comment [here](https://github.com/JamesSample/icpw2/issues/4#issuecomment-1523093566) for further discussion of this.\n",
    "\n",
    "The code below estimates the ion balance as\n",
    "\n",
    "$$IB = Ca + Mg + Na + K + HN4 + H - Cl - SO4 - NO3 - OrgAnions - HCO3$$\n",
    "\n",
    "and discards samples where the difference is greater than 20% of the summed cations. Note that because of the way we have calculated HCO3, this corresponds to samples where the initial HCO3 estimate was strongly negative.\n",
    "\n",
    "**Note:** After further discussion, we have decided not to use this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334cfa6a-c603-46b5-a162-31fcfc04305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter samples based on ion balance\n",
    "# for par in [\"NH4-N_µeq/l\", \"K_µeq/l\"]:\n",
    "#     if par in wc_df.columns:\n",
    "#         wc_df[par + \"_temp\"] = wc_df[par].fillna(0)\n",
    "#     else:\n",
    "#         wc_df[par + \"_temp\"] = 0\n",
    "\n",
    "# wc_df[\"Cations_µeq/l\"] = (\n",
    "#     wc_df[\"Ca_µeq/l\"]\n",
    "#     + wc_df[\"Mg_µeq/l\"]\n",
    "#     + wc_df[\"Na_µeq/l\"]\n",
    "#     + wc_df[\"K_µeq/l_temp\"]\n",
    "#     + wc_df[\"NH4-N_µeq/l_temp\"]\n",
    "#     + wc_df[\"H_µeq/l\"]\n",
    "# )\n",
    "# wc_df[\"Anions_µeq/l\"] = (\n",
    "#     wc_df[\"Cl_µeq/l\"]\n",
    "#     + wc_df[\"SO4_µeq/l\"]\n",
    "#     + wc_df[\"NO3-N_µeq/l\"]\n",
    "#     + wc_df[\"OrgAnions_µeq/l\"]\n",
    "#     + wc_df[\"HCO3_µeq/l\"]\n",
    "# )\n",
    "\n",
    "# wc_df[\"IonBal_µeq/l\"] = wc_df[\"Cations_µeq/l\"] - wc_df[\"Anions_µeq/l\"]\n",
    "# wc_df[\"Zdiff_pct\"] = 100 * wc_df[\"IonBal_µeq/l\"] / wc_df[\"Cations_µeq/l\"]\n",
    "# del wc_df[\"K_µeq/l_temp\"], wc_df[\"NH4-N_µeq/l_temp\"]\n",
    "\n",
    "# print(\"Total number of water samples:\", len(wc_df))\n",
    "# print(\n",
    "#     \"Number for which ion balance cannot be calculated:\",\n",
    "#     len(wc_df[pd.isna(wc_df[\"Zdiff_pct\"])]),\n",
    "# )\n",
    "# print(\n",
    "#     \"Number with ion balance difference >20%\", len(wc_df[wc_df[\"Zdiff_pct\"].abs() > 20])\n",
    "# )\n",
    "# print(\n",
    "#     \"Number with ion balance difference ≤20%\",\n",
    "#     len(wc_df[wc_df[\"Zdiff_pct\"].abs() <= 20]),\n",
    "# )\n",
    "\n",
    "# # Remove data with suspicious ion balance\n",
    "# wc_df = wc_df[wc_df[\"Zdiff_pct\"].abs() <= 20]\n",
    "# del (\n",
    "#     wc_df[\"Cations_µeq/l\"],\n",
    "#     wc_df[\"Anions_µeq/l\"],\n",
    "#     wc_df[\"IonBal_µeq/l\"],\n",
    "#     wc_df[\"Zdiff_pct\"],\n",
    "# )\n",
    "\n",
    "# print(\"Number of samples remaining:\", len(wc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad8dd7-37aa-42b2-a935-76bd7924708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated distribution plots\n",
    "# Join country attribute\n",
    "df = pd.merge(wc_df, stn_df[[\"station_id\", \"country\"]], how=\"left\", on=\"station_id\")\n",
    "\n",
    "# Pars to plot\n",
    "pars = [col for col in df.columns if \"eq/l\" in col] + [\"TOC_mg C/l\"]\n",
    "\n",
    "# Reshape\n",
    "df = df[[\"country\"] + pars]\n",
    "df = df.melt(id_vars=[\"country\"])\n",
    "\n",
    "# Plot\n",
    "g = sn.catplot(\n",
    "    data=df,\n",
    "    x=\"country\",\n",
    "    y=\"value\",\n",
    "    row=\"variable\",\n",
    "    kind=\"box\",\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    height=4,\n",
    "    aspect=3,\n",
    ")\n",
    "g.set_xticklabels(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da56a32-66d4-4f93-95c6-d6ca8a2da2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for speed later\n",
    "csv_path = r\"./data/thematic_report_2023_working_data.csv\"\n",
    "wc_df.to_csv(csv_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7997c-3e3e-4ee4-951e-9b4754feeb6b",
   "metadata": {},
   "source": [
    "## 5. Aggregate to annual medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36a9df-c3ed-421b-8c51-851091710f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved data\n",
    "csv_path = r\"./data/thematic_report_2023_working_data.csv\"\n",
    "wc_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "wc_df[\"sample_date\"] = pd.to_datetime(wc_df[\"sample_date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Annual medians by station\n",
    "wc_df[\"year\"] = wc_df[\"sample_date\"].dt.year\n",
    "ann_df = wc_df.groupby([\"station_id\", \"year\"]).median().reset_index()\n",
    "ann_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98f3b2-5186-422a-8793-33599f1d0076",
   "metadata": {},
   "source": [
    "## 6. Site selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05294a-27b9-4140-985b-9e64732afb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pars to consider\n",
    "df = ann_df.copy()\n",
    "pars = [col for col in df.columns if \"eq/l\" in col] + [\"TOC_mg C/l\"]\n",
    "df = df[[\"station_id\", \"year\"] + pars]\n",
    "\n",
    "# Melt to long format\n",
    "df = pd.melt(df, id_vars=[\"station_id\", \"year\"])\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815afb6-16ff-4007-a0b7-e2473609a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selection criteria\n",
    "n_start_thresh = 1  # Number of annual values in first 5 years\n",
    "n_end_thresh = 1  # Number of annual values in last 5 years\n",
    "prop_thresh = 0.65  # Proportion of total years with data\n",
    "periods = ((1990, 2020), (1990, 2004), (1998, 2012), (2006, 2020))\n",
    "\n",
    "# Dict for results\n",
    "inc_dict = {\n",
    "    \"station_id\": [],\n",
    "    \"variable\": [],\n",
    "    \"period\": [],\n",
    "    \"include\": [],\n",
    "}\n",
    "\n",
    "# Loop over time series\n",
    "for stn_id in df[\"station_id\"].unique():\n",
    "    # Loop over variables\n",
    "    for par in df[\"variable\"].unique():\n",
    "        # Get data\n",
    "        stn_par_df = df.query(\"(station_id == @stn_id) and (variable == @par)\")\n",
    "        stn_par_df.set_index(\"year\", inplace=True)\n",
    "        del stn_par_df[\"station_id\"], stn_par_df[\"variable\"]\n",
    "\n",
    "        for period in periods:\n",
    "            st_yr, end_yr = period\n",
    "            years = range(st_yr, end_yr + 1)\n",
    "            n_yrs = len(years)\n",
    "            yrs_thresh = round(n_yrs * prop_thresh)\n",
    "            years_df = pd.DataFrame(index=years)\n",
    "\n",
    "            # Join by year (=> annual series with no gaps)\n",
    "            stn_par_yr_df = years_df.join(stn_par_df)\n",
    "\n",
    "            if pd.isna(stn_par_yr_df[\"value\"]).all().all():\n",
    "                # Not suitable\n",
    "                inc_dict[\"station_id\"].append(stn_id)\n",
    "                inc_dict[\"variable\"].append(par)\n",
    "                inc_dict[\"period\"].append(f\"{st_yr}-{end_yr}\")\n",
    "                inc_dict[\"include\"].append(0)\n",
    "\n",
    "            else:\n",
    "                n_start = pd.notnull(\n",
    "                    stn_par_yr_df[stn_par_yr_df.index < (st_yr + 5)][\"value\"]\n",
    "                ).sum()\n",
    "                n_end = pd.notnull(\n",
    "                    stn_par_yr_df[stn_par_yr_df.index > (end_yr - 5)][\"value\"]\n",
    "                ).sum()\n",
    "                non_missing = pd.notnull(stn_par_yr_df[\"value\"]).sum()\n",
    "\n",
    "                if (\n",
    "                    (n_start >= n_start_thresh)\n",
    "                    and (n_end >= n_end_thresh)\n",
    "                    and (non_missing >= yrs_thresh)\n",
    "                ):\n",
    "                    # Include\n",
    "                    inc_dict[\"station_id\"].append(stn_id)\n",
    "                    inc_dict[\"variable\"].append(par)\n",
    "                    inc_dict[\"period\"].append(f\"{st_yr}-{end_yr}\")\n",
    "                    inc_dict[\"include\"].append(1)\n",
    "\n",
    "                else:\n",
    "                    # Not suitable\n",
    "                    inc_dict[\"station_id\"].append(stn_id)\n",
    "                    inc_dict[\"variable\"].append(par)\n",
    "                    inc_dict[\"period\"].append(f\"{st_yr}-{end_yr}\")\n",
    "                    inc_dict[\"include\"].append(0)\n",
    "\n",
    "# Build df\n",
    "inc_df = pd.DataFrame(inc_dict)\n",
    "\n",
    "# Unstack to 'wide'\n",
    "inc_df.set_index([\"station_id\", \"variable\", \"period\"], inplace=True)\n",
    "inc_df = inc_df.unstack(\"variable\")\n",
    "inc_df.columns = inc_df.columns.get_level_values(1)\n",
    "inc_df.reset_index(inplace=True)\n",
    "\n",
    "# Join station details\n",
    "inc_df = pd.merge(\n",
    "    inc_df,\n",
    "    stn_df[[\"station_id\", \"station_code\", \"station_name\", \"country\"]],\n",
    "    how=\"left\",\n",
    "    on=\"station_id\",\n",
    ")\n",
    "\n",
    "inc_df.to_csv(\"./data/selection_criteria_by_station-par-period.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0401e5-1ccb-4d7e-b45a-f6fc0c7f0662",
   "metadata": {},
   "source": [
    "## 7. Output raw data for each period\n",
    "\n",
    "Rolf would like two versions of the raw data: one where the **number of sites varies between periods** and another where it **remains fixed** (see e-mail received 16.05.2023 at 14.25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f88b3c-e171-4390-b15a-2f3a7d453a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of interest for analysis\n",
    "par_list = [\n",
    "    \"SO4_µeq/l\",\n",
    "    \"NO3-N_µeq/l\",\n",
    "    # \"NH4-N_µeq/l\",\n",
    "    \"Cl_µeq/l\",\n",
    "    \"Ca_µeq/l\",\n",
    "    \"Mg_µeq/l\",\n",
    "    \"CaMg_µeq/l\",\n",
    "    \"H_µeq/l\",\n",
    "    \"SAA_µeq/l\",\n",
    "    \"ANC_µeq/l\",\n",
    "    \"OrgAnions_µeq/l\",\n",
    "    \"HCO3_µeq/l\",\n",
    "    \"TOC_mg C/l\",\n",
    "]\n",
    "\n",
    "# Periods to consider\n",
    "periods = ((1990, 2020), (1990, 2004), (1998, 2012), (2006, 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9770e-748c-46db-96e8-9bcb8e9db4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Station propertiess\n",
    "xl_path = r\"../../../all_icpw_sites_mar_2023.xlsx\"\n",
    "stn_df = pd.read_excel(xl_path, sheet_name=\"all_icpw_stns\")\n",
    "\n",
    "# Median annual chemistry\n",
    "csv_path = r\"./data/thematic_report_2023_working_data.csv\"\n",
    "wc_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "wc_df[\"sample_date\"] = pd.to_datetime(wc_df[\"sample_date\"], format=\"%Y-%m-%d\")\n",
    "# wc_df[\"year\"] = wc_df[\"sample_date\"].dt.year\n",
    "# wc_df = wc_df.groupby([\"station_id\", \"year\"]).median().reset_index()\n",
    "wc_df = wc_df[[\"station_id\", \"year\"] + par_list]\n",
    "\n",
    "# Selection criteria\n",
    "csv_path = r\"./data/selection_criteria_by_station-par-period.csv\"\n",
    "inc_df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "inc_df = inc_df[[\"station_id\", \"period\"] + par_list]\n",
    "# del inc_df[\"NH4-N_µeq/l\"] # Don't care about NH4 for site selection - see above\n",
    "inc_df.set_index([\"station_id\", \"period\"], inplace=True)\n",
    "inc_df[inc_df == 0] = np.nan\n",
    "inc_df.dropna(how=\"any\", inplace=True)\n",
    "inc_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add80fc8-f744-4c18-bd3b-535de341396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vary_sites in (True, False):\n",
    "    if vary_sites:\n",
    "        xl_path = r\"./data/cleaned_icpw_data_vary_nsites.xlsx\"\n",
    "    else:\n",
    "        xl_path = r\"./data/cleaned_icpw_data_fixed_nsites.xlsx\"\n",
    "\n",
    "    with pd.ExcelWriter(xl_path) as writer:\n",
    "        # Get station list if nsites is fixed\n",
    "        stn_list = (\n",
    "            inc_df.groupby([\"station_id\"])[[\"period\"]]\n",
    "            .count()\n",
    "            .query(\"period == 4\")\n",
    "            .index.tolist()\n",
    "        )\n",
    "        for period in periods:\n",
    "            st_yr, end_yr = period\n",
    "\n",
    "            # Allow sites list to change by period, if desired\n",
    "            if vary_sites:\n",
    "                inc_df_per = inc_df.query(f\"period == '{st_yr}-{end_yr}'\").copy()\n",
    "                stn_list = list(inc_df_per[\"station_id\"].unique())\n",
    "\n",
    "            # Get data that meet selection criteria for all parameters of interest\n",
    "            df_per = wc_df.query(\n",
    "                \"(station_id in @stn_list) and (@st_yr <= year <= @end_yr)\"\n",
    "            ).copy()\n",
    "\n",
    "            # Save to Excel\n",
    "            df_per.to_excel(writer, sheet_name=f\"{st_yr}-{end_yr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
